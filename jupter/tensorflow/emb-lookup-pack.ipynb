{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1,2,3,4],[5,6,7,8],[0,0,0,0], [1,1,1,1]], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(x, [0, 2]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.,  3.,  4.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 5.,  6.,  7.,  8.],\n",
       "        [ 1.,  1.,  1.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(x, [[0, 2], [1,3]]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(x, [[0, 2], [1,3]]).eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(x, [0, 2]).eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " 'absolute_import',\n",
       " 'bayesflow',\n",
       " 'copy_graph',\n",
       " 'distributions',\n",
       " 'division',\n",
       " 'factorization',\n",
       " 'framework',\n",
       " 'graph_editor',\n",
       " 'grid_rnn',\n",
       " 'layers',\n",
       " 'learn',\n",
       " 'linear_optimizer',\n",
       " 'lookup',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'opt',\n",
       " 'print_function',\n",
       " 'quantization',\n",
       " 'rnn',\n",
       " 'session_bundle',\n",
       " 'slim',\n",
       " 'tensor_forest',\n",
       " 'testing',\n",
       " 'util']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name skflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-72e70e218e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name skflow"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn as skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split_squeeze in module tensorflow.contrib.framework.python.framework.deprecation:\n",
      "\n",
      "split_squeeze(*args, **kwargs)\n",
      "    Splits input on given dimension and then squeezes that dimension. (deprecated)\n",
      "    \n",
      "    THIS FUNCTION IS DEPRECATED. It will be removed after 2016-08-01.\n",
      "    Instructions for updating:\n",
      "    Please use tf.unpack instead.\n",
      "    \n",
      "      Args:\n",
      "        dim: Dimension to split and squeeze on.\n",
      "        num_split: integer, the number of ways to split.\n",
      "        tensor_in: Input tensor of shape [N1, N2, .. Ndim, .. Nx].\n",
      "    \n",
      "      Returns:\n",
      "        List of tensors [N1, N2, .. Ndim-1, Ndim+1, .. Nx].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(skflow.ops.split_squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0rc0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function unpack in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "unpack(value, num=None, axis=0, name='unpack')\n",
      "    Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n",
      "    \n",
      "    Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\n",
      "    If `num` is not specified (the default), it is inferred from `value`'s shape.\n",
      "    If `value.shape[axis]` is not known, `ValueError` is raised.\n",
      "    \n",
      "    For example, given a tensor of shape `(A, B, C, D)`;\n",
      "    \n",
      "    If `axis == 0` then the i'th tensor in `output` is the slice\n",
      "      `value[i, :, :, :]` and each tensor in `output` will have shape `(B, C, D)`.\n",
      "      (Note that the dimension unpacked along is gone, unlike `split`).\n",
      "    \n",
      "    If `axis == 1` then the i'th tensor in `output` is the slice\n",
      "      `value[:, i, :, :]` and each tensor in `output` will have shape `(A, C, D)`.\n",
      "    Etc.\n",
      "    \n",
      "    This is the opposite of pack.  The numpy equivalent is\n",
      "    \n",
      "        tf.unpack(x, n) = list(x)\n",
      "    \n",
      "    Args:\n",
      "      value: A rank `R > 0` `Tensor` to be unpacked.\n",
      "      num: An `int`. The length of the dimension `axis`. Automatically inferred\n",
      "        if `None` (the default).\n",
      "      axis: An `int`. The axis to unpack along. Defaults to the first\n",
      "        dimension. Supports negative indexes.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The list of `Tensor` objects unpacked from `value`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `num` is unspecified and cannot be inferred.\n",
      "      ValueError: If `axis` is out of the range [-R, R).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.unpack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1  = tf.unpack(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unpack_17:0' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_17:1' shape=(4,) dtype=float32>]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print l1\n",
    "print len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unpack_19:0' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:1' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:2' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:3' shape=(4,) dtype=float32>]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "u = tf.unpack(x)\n",
    "print u\n",
    "print len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unpack_17:0' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_17:1' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:0' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:1' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:2' shape=(4,) dtype=float32>, <tf.Tensor 'unpack_19:3' shape=(4,) dtype=float32>]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "l1 = l1 + u\n",
    "print l1\n",
    "print len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2 = tf.pack(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pack_6:0' shape=(6, 4) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 5.,  6.,  7.,  8.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(4, 4) dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, 0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function arg_max in module tensorflow.python.ops.gen_math_ops:\n",
      "\n",
      "arg_max(input, dimension, name=None)\n",
      "    Returns the index with the largest value across dimensions of a tensor.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`, `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n",
      "      dimension: A `Tensor` of type `int32`.\n",
      "        int32, 0 <= dimension < rank(input).  Describes which dimension\n",
      "        of the input Tensor to reduce across. For vectors, use dimension = 0.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `int64`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.top_k(x, 2)[1][:,1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [3, 2],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.top_k(x, 2)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function top_k in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "top_k(input, k=1, sorted=True, name=None)\n",
      "    Finds values and indices of the `k` largest entries for the last dimension.\n",
      "    \n",
      "    If the input is a vector (rank-1), finds the `k` largest entries in the vector\n",
      "    and outputs their values and indices as vectors.  Thus `values[j]` is the\n",
      "    `j`-th largest entry in `input`, and its index is `indices[j]`.\n",
      "    \n",
      "    For matrices (resp. higher rank input), computes the top `k` entries in each\n",
      "    row (resp. vector along the last dimension).  Thus,\n",
      "    \n",
      "        values.shape = indices.shape = input.shape[:-1] + [k]\n",
      "    \n",
      "    If two elements are equal, the lower-index element appears first.\n",
      "    \n",
      "    Args:\n",
      "      input: 1-D or higher `Tensor` with last dimension at least `k`.\n",
      "      k: 0-D `int32` `Tensor`.  Number of top elements to look for along the last\n",
      "        dimension (along each row for matrices).\n",
      "      sorted: If true the resulting `k` elements will be sorted by the values in\n",
      "        descending order.\n",
      "      name: Optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      values: The `k` largest elements along each last dimensional slice.\n",
      "      indices: The indices of `values` within the last dimension of `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function choice:\n",
      "\n",
      "choice(...)\n",
      "    choice(a, size=None, replace=True, p=None)\n",
      "    \n",
      "    Generates a random sample from a given 1-D array\n",
      "    \n",
      "            .. versionadded:: 1.7.0\n",
      "    \n",
      "    Parameters\n",
      "    -----------\n",
      "    a : 1-D array-like or int\n",
      "        If an ndarray, a random sample is generated from its elements.\n",
      "        If an int, the random sample is generated as if a was np.arange(n)\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
      "        single value is returned.\n",
      "    replace : boolean, optional\n",
      "        Whether the sample is with or without replacement\n",
      "    p : 1-D array-like, optional\n",
      "        The probabilities associated with each entry in a.\n",
      "        If not given the sample assumes a uniform distribution over all\n",
      "        entries in a.\n",
      "    \n",
      "    Returns\n",
      "    --------\n",
      "    samples : 1-D ndarray, shape (size,)\n",
      "        The generated random samples\n",
      "    \n",
      "    Raises\n",
      "    -------\n",
      "    ValueError\n",
      "        If a is an int and less than zero, if a or p are not 1-dimensional,\n",
      "        if a is an array-like of size 0, if p is not a vector of\n",
      "        probabilities, if a and p have different lengths, or if\n",
      "        replace=False and the sample size is greater than the population\n",
      "        size\n",
      "    \n",
      "    See Also\n",
      "    ---------\n",
      "    randint, shuffle, permutation\n",
      "    \n",
      "    Examples\n",
      "    ---------\n",
      "    Generate a uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3)\n",
      "    array([0, 3, 4])\n",
      "    >>> #This is equivalent to np.random.randint(0,5,3)\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([3, 3, 0])\n",
      "    \n",
      "    Generate a uniform random sample from np.arange(5) of size 3 without\n",
      "    replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False)\n",
      "    array([3,1,0])\n",
      "    >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size\n",
      "    3 without replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([2, 3, 0])\n",
      "    \n",
      "    Any of the above can be repeated with an arbitrary array-like\n",
      "    instead of just integers. For instance:\n",
      "    \n",
      "    >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
      "    >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\n",
      "    array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'],\n",
      "          dtype='|S11')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.random.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1, 1, 1]\n",
      "    #         [1, 1, 1]]\n",
      "    tf.reduce_sum(x) ==> 6\n",
      "    tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
      "    tf.reduce_sum(x, 1) ==> [3, 3]\n",
      "    tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
      "    tf.reduce_sum(x, [0, 1]) ==> 6\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '_compute_sampled_logits',\n",
       " '_sum_rows',\n",
       " 'absolute_import',\n",
       " 'all_candidate_sampler',\n",
       " 'array_ops',\n",
       " 'atrous_conv2d',\n",
       " 'avg_pool',\n",
       " 'avg_pool3d',\n",
       " 'avg_pool3d_grad',\n",
       " 'batch_norm_with_global_normalization',\n",
       " 'batch_normalization',\n",
       " 'bias_add',\n",
       " 'bias_add_grad',\n",
       " 'bias_add_v1',\n",
       " 'bidirectional_dynamic_rnn',\n",
       " 'bidirectional_rnn',\n",
       " 'candidate_sampling_ops',\n",
       " 'collections',\n",
       " 'common_shapes',\n",
       " 'compute_accidental_hits',\n",
       " 'constant_op',\n",
       " 'control_flow_ops',\n",
       " 'conv1d',\n",
       " 'conv2d',\n",
       " 'conv2d_backprop_filter',\n",
       " 'conv2d_backprop_input',\n",
       " 'conv2d_transpose',\n",
       " 'conv3d',\n",
       " 'conv3d_backprop_filter',\n",
       " 'conv3d_backprop_filter_v2',\n",
       " 'conv3d_backprop_input',\n",
       " 'conv3d_backprop_input_v2',\n",
       " 'conv3d_transpose',\n",
       " 'ctc_beam_search_decoder',\n",
       " 'ctc_greedy_decoder',\n",
       " 'ctc_loss',\n",
       " 'data_flow_ops',\n",
       " 'depthwise_conv2d',\n",
       " 'depthwise_conv2d_native',\n",
       " 'depthwise_conv2d_native_backprop_filter',\n",
       " 'depthwise_conv2d_native_backprop_input',\n",
       " 'dilation2d',\n",
       " 'dilation2d_backprop_filter',\n",
       " 'dilation2d_backprop_input',\n",
       " 'division',\n",
       " 'dropout',\n",
       " 'dtypes',\n",
       " 'dynamic_rnn',\n",
       " 'elu',\n",
       " 'embedding_lookup',\n",
       " 'embedding_lookup_sparse',\n",
       " 'embedding_ops',\n",
       " 'erosion2d',\n",
       " 'fixed_unigram_candidate_sampler',\n",
       " 'gen_candidate_sampling_ops',\n",
       " 'gen_ctc_ops',\n",
       " 'gen_nn_ops',\n",
       " 'graph_util',\n",
       " 'in_top_k',\n",
       " 'init_ops',\n",
       " 'l2_loss',\n",
       " 'l2_normalize',\n",
       " 'learned_unigram_candidate_sampler',\n",
       " 'local_response_normalization',\n",
       " 'log_softmax',\n",
       " 'log_uniform_candidate_sampler',\n",
       " 'logging_ops',\n",
       " 'lrn',\n",
       " 'make_all',\n",
       " 'math_ops',\n",
       " 'max_pool',\n",
       " 'max_pool3d',\n",
       " 'max_pool3d_grad',\n",
       " 'max_pool_with_argmax',\n",
       " 'moments',\n",
       " 'nce_loss',\n",
       " 'nest',\n",
       " 'nn_grad',\n",
       " 'nn_ops',\n",
       " 'normalize_moments',\n",
       " 'np',\n",
       " 'numerics',\n",
       " 'op_def_library',\n",
       " 'op_def_pb2',\n",
       " 'op_def_registry',\n",
       " 'ops',\n",
       " 'print_function',\n",
       " 'random_ops',\n",
       " 'random_seed',\n",
       " 'raw_rnn',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'relu_layer',\n",
       " 'rnn',\n",
       " 'rnn_cell',\n",
       " 'sampled_softmax_loss',\n",
       " 'separable_conv2d',\n",
       " 'seq2seq',\n",
       " 'sigmoid',\n",
       " 'sigmoid_cross_entropy_with_logits',\n",
       " 'softmax',\n",
       " 'softmax_cross_entropy_with_logits',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'sparse_ops',\n",
       " 'sparse_softmax_cross_entropy_with_logits',\n",
       " 'state_saving_rnn',\n",
       " 'sufficient_statistics',\n",
       " 'tanh',\n",
       " 'tensor_array_ops',\n",
       " 'tensor_shape',\n",
       " 'tensor_util',\n",
       " 'text_format',\n",
       " 'top_k',\n",
       " 'uniform_candidate_sampler',\n",
       " 'vs',\n",
       " 'weighted_cross_entropy_with_logits',\n",
       " 'xrange',\n",
       " 'xw_plus_b',\n",
       " 'xw_plus_b_v1',\n",
       " 'zero_fraction']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function xw_plus_b in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "xw_plus_b(x, weights, biases, name=None)\n",
      "    Computes matmul(x, weights) + biases.\n",
      "    \n",
      "    Args:\n",
      "      x: a 2D tensor.  Dimensions typically: batch, in_units\n",
      "      weights: a 2D tensor.  Dimensions typically: in_units, out_units\n",
      "      biases: a 1D tensor.  Dimensions: out_units\n",
      "      name: A name for the operation (optional).  If not specified\n",
      "        \"xw_plus_b\" is used.\n",
      "    \n",
      "    Returns:\n",
      "      A 2-D Tensor computing matmul(x, weights) + biases.\n",
      "      Dimensions typically: batch, out_units.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.xw_plus_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(__builtin__.object)\n",
      " |  See the [Variables How To](../../how_tos/variables/index.md) for a high\n",
      " |  level overview.\n",
      " |  \n",
      " |  A variable maintains state in the graph across calls to `run()`. You add a\n",
      " |  variable to the graph by constructing an instance of the class `Variable`.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  If you want to change the shape of a variable later you have to use an\n",
      " |  `assign` Op with `validate_shape=False`.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs for other Ops in the graph. Additionally, all the operators\n",
      " |  overloaded for the `Tensor` class are carried over to variables, so you can\n",
      " |  also add nodes to the graph by just doing arithmetic on variables.\n",
      " |  \n",
      " |  ```python\n",
      " |  import tensorflow as tf\n",
      " |  \n",
      " |  # Create a variable.\n",
      " |  w = tf.Variable(<initial-value>, name=<optional-name>)\n",
      " |  \n",
      " |  # Use the variable in the graph like any Tensor.\n",
      " |  y = tf.matmul(w, ...another variable or tensor...)\n",
      " |  \n",
      " |  # The overloaded operators are available too.\n",
      " |  z = tf.sigmoid(w + y)\n",
      " |  \n",
      " |  # Assign a new value to the variable with `assign()` or a related method.\n",
      " |  w.assign(w + 1.0)\n",
      " |  w.assign_add(1.0)\n",
      " |  ```\n",
      " |  \n",
      " |  When you launch the graph, variables have to be explicitly initialized before\n",
      " |  you can run Ops that use their value. You can initialize a variable by\n",
      " |  running its *initializer op*, restoring the variable from a save file, or\n",
      " |  simply running an `assign` Op that assigns a value to the variable. In fact,\n",
      " |  the variable *initializer op* is just an `assign` Op that assigns the\n",
      " |  variable's initial value to the variable itself.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the variable initializer.\n",
      " |      sess.run(w.initializer)\n",
      " |      # ...you now can run ops that use the value of 'w'...\n",
      " |  ```\n",
      " |  \n",
      " |  The most common initialization pattern is to use the convenience function\n",
      " |  `initialize_all_variables()` to add an Op to the graph that initializes\n",
      " |  all the variables. You then run that Op after launching the graph.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Add an Op to initialize all variables.\n",
      " |  init_op = tf.initialize_all_variables()\n",
      " |  \n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the Op that initializes all variables.\n",
      " |      sess.run(init_op)\n",
      " |      # ...you can now run any Op that uses variable values...\n",
      " |  ```\n",
      " |  \n",
      " |  If you need to create a variable with an initial value dependent on another\n",
      " |  variable, use the other variable's `initialized_value()`. This ensures that\n",
      " |  variables are initialized in the right order.\n",
      " |  \n",
      " |  All variables are automatically collected in the graph where they are\n",
      " |  created. By default, the constructor adds the new variable to the graph\n",
      " |  collection `GraphKeys.VARIABLES`. The convenience function\n",
      " |  `all_variables()` returns the contents of that collection.\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  betwen variables holding the trainable model parameters and other variables\n",
      " |  such as a `global step` variable used to count training steps. To make this\n",
      " |  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
      " |  `True`, the new variable is also added to the graph collection\n",
      " |  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
      " |  `trainable_variables()` returns the contents of this collection. The\n",
      " |  various `Optimizer` classes use this collection as the default list of\n",
      " |  variables to optimize.\n",
      " |  \n",
      " |  \n",
      " |  Creating a variable.\n",
      " |  \n",
      " |  @@__init__\n",
      " |  @@initialized_value\n",
      " |  \n",
      " |  Changing a variable value.\n",
      " |  \n",
      " |  @@assign\n",
      " |  @@assign_add\n",
      " |  @@assign_sub\n",
      " |  @@scatter_sub\n",
      " |  @@count_up_to\n",
      " |  \n",
      " |  @@eval\n",
      " |  \n",
      " |  Properties.\n",
      " |  \n",
      " |  @@name\n",
      " |  @@dtype\n",
      " |  @@get_shape\n",
      " |  @@device\n",
      " |  @@initializer\n",
      " |  @@graph\n",
      " |  @@op\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ lambda a\n",
      " |  \n",
      " |  __add__ lambda a, b\n",
      " |  \n",
      " |  __and__ lambda a, b\n",
      " |  \n",
      " |  __div__ lambda a, b\n",
      " |  \n",
      " |  __floordiv__ lambda a, b\n",
      " |  \n",
      " |  __ge__ lambda a, b\n",
      " |  \n",
      " |  __getitem__ lambda a, b\n",
      " |  \n",
      " |  __gt__ lambda a, b\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None)\n",
      " |      Creates a new variable with value `initial_value`.\n",
      " |      \n",
      " |      The new variable is added to the graph collections listed in `collections`,\n",
      " |      which defaults to `[GraphKeys.VARIABLES]`.\n",
      " |      \n",
      " |      If `trainable` is `True` the variable is also added to the graph collection\n",
      " |      `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |      \n",
      " |      This constructor creates both a `variable` Op and an `assign` Op to set the\n",
      " |      variable to its initial value.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.VARIABLES]`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device.\n",
      " |          If not `None`, caches on another device.  Typical use is to cache\n",
      " |          on the device where the Ops using the Variable reside, to deduplicate\n",
      " |          copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates\n",
      " |          the Variable object with its contents. `variable_def` and the other\n",
      " |          arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If `None`, either the datatype will be kept (if `initial_value` is\n",
      " |          a Tensor), or `convert_to_tensor` will decide.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A Variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |  \n",
      " |  __invert__ lambda a\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration. Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ lambda a, b\n",
      " |  \n",
      " |  __lt__ lambda a, b\n",
      " |  \n",
      " |  __mod__ lambda a, b\n",
      " |  \n",
      " |  __mul__ lambda a, b\n",
      " |  \n",
      " |  __neg__ lambda a\n",
      " |  \n",
      " |  __or__ lambda a, b\n",
      " |  \n",
      " |  __pow__ lambda a, b\n",
      " |  \n",
      " |  __radd__ lambda a, b\n",
      " |  \n",
      " |  __rand__ lambda a, b\n",
      " |  \n",
      " |  __rdiv__ lambda a, b\n",
      " |  \n",
      " |  __rfloordiv__ lambda a, b\n",
      " |  \n",
      " |  __rmod__ lambda a, b\n",
      " |  \n",
      " |  __rmul__ lambda a, b\n",
      " |  \n",
      " |  __ror__ lambda a, b\n",
      " |  \n",
      " |  __rpow__ lambda a, b\n",
      " |  \n",
      " |  __rsub__ lambda a, b\n",
      " |  \n",
      " |  __rtruediv__ lambda a, b\n",
      " |  \n",
      " |  __rxor__ lambda a, b\n",
      " |  \n",
      " |  __sub__ lambda a, b\n",
      " |  \n",
      " |  __truediv__ lambda a, b\n",
      " |  \n",
      " |  __xor__ lambda a, b\n",
      " |  \n",
      " |  assign(self, value, use_locking=False)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph containing this\n",
      " |      variable has been launched. If no session is passed, the default session is\n",
      " |      used.  See the [Session class](../../api_docs/python/client.md#Session) for\n",
      " |      more information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.initialize_all_variables()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If\n",
      " |          none, the default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  ref(self)\n",
      " |      Returns a reference to this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need a reference\n",
      " |      to the variable call it automatically.\n",
      " |      \n",
      " |      Returns is a `Tensor` which holds a reference to the variable.  You can\n",
      " |      assign a new value to the variable by passing the tensor to an assign op.\n",
      " |      See [`value()`](#Variable.value) if you want to get the value of the\n",
      " |      variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that is a reference to the variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False)\n",
      " |      Subtracts `IndexedSlices` from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `scatter_sub(self, sparse_delta.indices,\n",
      " |      sparse_delta.values)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  to_proto(self)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      See [`ref()`](#Variable.ref) if you want to get a reference to the\n",
      " |      variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.SaveSliceInfo'...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,  12.,  18.,  24.],\n",
       "       [ 22.,  28.,  34.,  40.],\n",
       "       [  2.,   4.,   6.,   8.],\n",
       "       [  6.,   8.,  10.,  12.]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,  12.,  18.,  24.],\n",
       "       [ 22.,  28.,  34.,  40.],\n",
       "       [  2.,   4.,   6.,   8.],\n",
       "       [  6.,   8.,  10.,  12.]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,   6.,   9.,  12.],\n",
       "       [ 11.,  14.,  17.,  20.],\n",
       "       [  1.,   2.,   3.,   4.],\n",
       "       [  3.,   4.,   5.,   6.]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.constant([1,2,3,4], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,   6.,   9.,  12.],\n",
       "       [ 11.,  14.,  17.,  20.],\n",
       "       [  1.,   2.,   3.,   4.],\n",
       "       [  3.,   4.,   5.,   6.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1,2,3,4],[5,6,7,8],[0,0,0,0], [1,1,1,1]], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x + tf.reshape(y, [4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,   3.,   4.,   5.],\n",
       "       [  7.,   8.,   9.,  10.],\n",
       "       [  3.,   3.,   3.,   3.],\n",
       "       [  5.,   5.,   5.,   5.]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_mean in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the mean of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1., 1.]\n",
      "    #         [2., 2.]]\n",
      "    tf.reduce_mean(x) ==> 1.5\n",
      "    tf.reduce_mean(x, 0) ==> [1.5, 1.5]\n",
      "    tf.reduce_mean(x, 1) ==> [1.,  2.]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5,  8.5,  3. ,  5. ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5,  8.5,  3. ,  5. ], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, -1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.,  34.,  12.,  20.], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14285715,  0.0882353 ,  0.33333334,  0.25      ],\n",
       "       [ 0.5       ,  0.23529412,  0.75      ,  0.5       ],\n",
       "       [ 0.21428572,  0.0882353 ,  0.25      ,  0.15000001],\n",
       "       [ 0.35714287,  0.14705883,  0.41666666,  0.25      ]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x / tf.reduce_sum(x, 1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  8.,  3.,  5.], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.],\n",
       "       [ 7.],\n",
       "       [ 3.],\n",
       "       [ 5.]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.slice(x, [0, 0], [-1, 1]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14285715,  0.05882353,  0.16666667,  0.1       ],\n",
       "       [ 0.5       ,  0.20588236,  0.58333331,  0.34999999],\n",
       "       [ 0.21428572,  0.0882353 ,  0.25      ,  0.15000001],\n",
       "       [ 0.35714287,  0.14705883,  0.41666666,  0.25      ]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.slice(x, [0, 0], [-1, 1]) / tf.reduce_sum(x, 1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1, 1, 1]\n",
      "    #         [1, 1, 1]]\n",
      "    tf.reduce_sum(x) ==> 6\n",
      "    tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
      "    tf.reduce_sum(x, 1) ==> [3, 3]\n",
      "    tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
      "    tf.reduce_sum(x, [0, 1]) ==> 6\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_softmax_cross_entropy_with_logits in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)\n",
      "    Computes sparse softmax cross entropy between `logits` and `labels`.\n",
      "    \n",
      "    Measures the probability error in discrete classification tasks in which the\n",
      "    classes are mutually exclusive (each entry is in exactly one class).  For\n",
      "    example, each CIFAR-10 image is labeled with one and only one label: an image\n",
      "    can be a dog or a truck, but not both.\n",
      "    \n",
      "    **NOTE:**  For this operation, the probability of a given label is considered\n",
      "    exclusive.  That is, soft classes are not allowed, and the `labels` vector\n",
      "    must provide a single specific index for the true class for each row of\n",
      "    `logits` (each minibatch entry).  For soft softmax classification with\n",
      "    a probability distribution for each entry, see\n",
      "    `softmax_cross_entropy_with_logits`.\n",
      "    \n",
      "    **WARNING:** This op expects unscaled logits, since it performs a softmax\n",
      "    on `logits` internally for efficiency.  Do not call this op with the\n",
      "    output of `softmax`, as it will produce incorrect results.\n",
      "    \n",
      "    A common use case is to have logits of shape `[batch_size, num_classes]` and\n",
      "    labels of shape `[batch_size]`. But higher dimensions are supported.\n",
      "    \n",
      "    Args:\n",
      "      logits: Unscaled log probabilities of rank `r` and shape\n",
      "        `[d_0, d_1, ..., d_{r-2}, num_classes]` and dtype `float32` or `float64`.\n",
      "      labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-2}]` and dtype `int32` or\n",
      "        `int64`. Each entry in `labels` must be an index in `[0, num_classes)`.\n",
      "        Other values will result in a loss of 0, but incorrect gradient\n",
      "        computations.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of the same shape as `labels` and of the same type as `logits`\n",
      "      with the softmax cross entropy loss.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If logits are scalars (need to have rank >= 1) or if the rank\n",
      "        of the labels is not equal to the rank of the labels minus one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.sparse_softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function in_top_k in module tensorflow.python.ops.gen_nn_ops:\n",
      "\n",
      "in_top_k(predictions, targets, k, name=None)\n",
      "    Says whether the targets are in the top `K` predictions.\n",
      "    \n",
      "    This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\n",
      "    prediction for the target class is among the top `k` predictions among\n",
      "    all predictions for example `i`. Note that the behavior of `InTopK` differs\n",
      "    from the `TopK` op in its handling of ties; if multiple classes have the\n",
      "    same prediction value and straddle the top-`k` boundary, all of those\n",
      "    classes are considered to be in the top `k`.\n",
      "    \n",
      "    More formally, let\n",
      "    \n",
      "      \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n",
      "      \\\\(targets_i\\\\) be the target class for example `i`,\n",
      "      \\\\(out_i\\\\) be the output for example `i`,\n",
      "    \n",
      "    $$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$\n",
      "    \n",
      "    Args:\n",
      "      predictions: A `Tensor` of type `float32`.\n",
      "        A `batch_size` x `classes` tensor.\n",
      "      targets: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        A `batch_size` vector of class ids.\n",
      "      k: An `int`. Number of top elements to look at for computing precision.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `bool`. Computed Precision at `k` as a `bool Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.in_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.Int64List"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
