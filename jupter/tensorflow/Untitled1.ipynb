{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', -1], ('a', 4), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "words = ['a', 'a', 'a', 'b', 'c', 'a', 'c', 'c', 'b', 'b']\n",
    "vocabulary_size = 3\n",
    "count = [['UNK', -1]]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "print count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'c': 2, 'UNK': 0}\n"
     ]
    }
   ],
   "source": [
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', 3], ('a', 4), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 2, 1, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'UNK', 1: 'a', 2: 'c'}\n"
     ]
    }
   ],
   "source": [
    " reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    " print reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_mean in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the mean of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1., 1.]\n",
      "    #         [2., 2.]]\n",
      "    tf.reduce_mean(x) ==> 1.5\n",
      "    tf.reduce_mean(x, 0) ==> [1.5, 1.5]\n",
      "    tf.reduce_mean(x, 1) ==> [1.,  2.]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help(tf.reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function nce_loss in module tensorflow.python.ops.nn:\n",
      "\n",
      "nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod', name='nce_loss')\n",
      "    Computes and returns the noise-contrastive estimation training loss.\n",
      "    \n",
      "    See [Noise-contrastive estimation: A new estimation principle for\n",
      "    unnormalized statistical models]\n",
      "    (http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf).\n",
      "    Also see our [Candidate Sampling Algorithms Reference]\n",
      "    (../../extras/candidate_sampling.pdf)\n",
      "    \n",
      "    Note: In the case where `num_true` > 1, we assign to each target class\n",
      "    the target probability 1 / `num_true` so that the target probabilities\n",
      "    sum to 1 per-example.\n",
      "    \n",
      "    Note: It would be useful to allow a variable number of target classes per\n",
      "    example.  We hope to provide this functionality in a future release.\n",
      "    For now, if you have a variable number of target classes, you can pad them\n",
      "    out to a constant number by either repeating them or by padding\n",
      "    with an otherwise unused class.\n",
      "    \n",
      "    Args:\n",
      "      weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n",
      "          objects whose concatenation along dimension 0 has shape\n",
      "          [num_classes, dim].  The (possibly-partitioned) class embeddings.\n",
      "      biases: A `Tensor` of shape `[num_classes]`.  The class biases.\n",
      "      inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n",
      "          activations of the input network.\n",
      "      labels: A `Tensor` of type `int64` and shape `[batch_size,\n",
      "          num_true]`. The target classes.\n",
      "      num_sampled: An `int`.  The number of classes to randomly sample per batch.\n",
      "      num_classes: An `int`. The number of possible classes.\n",
      "      num_true: An `int`.  The number of target classes per training example.\n",
      "      sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,\n",
      "          `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n",
      "          (if None, we default to `log_uniform_candidate_sampler`)\n",
      "      remove_accidental_hits:  A `bool`.  Whether to remove \"accidental hits\"\n",
      "          where a sampled class equals one of the target classes.  If set to\n",
      "          `True`, this is a \"Sampled Logistic\" loss instead of NCE, and we are\n",
      "          learning to generate log-odds instead of log probabilities.  See\n",
      "          our [Candidate Sampling Algorithms Reference]\n",
      "          (../../extras/candidate_sampling.pdf).\n",
      "          Default is False.\n",
      "      partition_strategy: A string specifying the partitioning strategy, relevant\n",
      "          if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n",
      "          Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `batch_size` 1-D tensor of per-example NCE losses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.nce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FixedLenFeature in module tensorflow.python.ops.parsing_ops:\n",
      "\n",
      "class FixedLenFeature(FixedLenFeature)\n",
      " |  Configuration for parsing a fixed-length input feature.\n",
      " |  \n",
      " |  To treat sparse input as dense, provide a `default_value`; otherwise,\n",
      " |  the parse functions will fail on any examples missing this feature.\n",
      " |  \n",
      " |  Fields:\n",
      " |    shape: Shape of input data.\n",
      " |    dtype: Data type of input.\n",
      " |    default_value: Value to be used if an example is missing this feature. It\n",
      " |        must be compatible with `dtype`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FixedLenFeature\n",
      " |      FixedLenFeature\n",
      " |      __builtin__.tuple\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from FixedLenFeature:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |      Return self as a plain tuple.  Used by copy and pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a nicely formatted representation string\n",
      " |  \n",
      " |  _asdict(self)\n",
      " |      Return a new OrderedDict which maps field names to their values\n",
      " |  \n",
      " |  _replace(_self, **kwds)\n",
      " |      Return a new FixedLenFeature object replacing specified fields with new values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from FixedLenFeature:\n",
      " |  \n",
      " |  _make(cls, iterable, new=<built-in method __new__ of type object>, len=<built-in function len>) from __builtin__.type\n",
      " |      Make a new FixedLenFeature object from a sequence or iterable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from FixedLenFeature:\n",
      " |  \n",
      " |  __new__(_cls, shape, dtype, default_value=None)\n",
      " |      Create new instance of FixedLenFeature(shape, dtype, default_value)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from FixedLenFeature:\n",
      " |  \n",
      " |  default_value\n",
      " |      Alias for field number 2\n",
      " |  \n",
      " |  dtype\n",
      " |      Alias for field number 1\n",
      " |  \n",
      " |  shape\n",
      " |      Alias for field number 0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from FixedLenFeature:\n",
      " |  \n",
      " |  _fields = ('shape', 'dtype', 'default_value')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from __builtin__.tuple:\n",
      " |  \n",
      " |  __add__(...)\n",
      " |      x.__add__(y) <==> x+y\n",
      " |  \n",
      " |  __contains__(...)\n",
      " |      x.__contains__(y) <==> y in x\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __getattribute__(...)\n",
      " |      x.__getattribute__('name') <==> x.name\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __getslice__(...)\n",
      " |      x.__getslice__(i, j) <==> x[i:j]\n",
      " |      \n",
      " |      Use of negative indices is not supported.\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __hash__(...)\n",
      " |      x.__hash__() <==> hash(x)\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      x.__iter__() <==> iter(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      x.__len__() <==> len(x)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __mul__(...)\n",
      " |      x.__mul__(n) <==> x*n\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __rmul__(...)\n",
      " |      x.__rmul__(n) <==> n*x\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      T.__sizeof__() -- size of T in memory, in bytes\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.FixedLenFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function string_input_producer in module tensorflow.python.training.input:\n",
      "\n",
      "string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)\n",
      "    Output strings (e.g. filenames) to a queue for an input pipeline.\n",
      "    \n",
      "    Args:\n",
      "      string_tensor: A 1-D string tensor with the strings to produce.\n",
      "      num_epochs: An integer (optional). If specified, `string_input_producer`\n",
      "        produces each string from `string_tensor` `num_epochs` times before\n",
      "        generating an `OutOfRange` error. If not specified,\n",
      "        `string_input_producer` can cycle through the strings in `string_tensor`\n",
      "        an unlimited number of times.\n",
      "      shuffle: Boolean. If true, the strings are randomly shuffled within each\n",
      "        epoch.\n",
      "      seed: An integer (optional). Seed used if shuffle == True.\n",
      "      capacity: An integer. Sets the queue capacity.\n",
      "      shared_name: (optional). If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: A name for the operations (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A queue with the output strings.  A `QueueRunner` for the Queue\n",
      "      is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the string_tensor is a null Python list.  At runtime,\n",
      "      will fail with an assertion if string_tensor becomes a null tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.string_input_producer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
