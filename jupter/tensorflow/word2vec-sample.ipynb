{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x9a5d250>> ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n",
      "[ 0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "emb_dim = 12\n",
    "init_width = 0.5 / emb_dim\n",
    "vocab_size = 4\n",
    "batch_size = 6\n",
    "emb = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [vocab_size, emb_dim], -init_width, init_width),\n",
    "        name=\"emb\")\n",
    "\n",
    "sm_w_t = tf.Variable(\n",
    "        tf.zeros([vocab_size, emb_dim]),\n",
    "        name=\"sm_w_t\")\n",
    "sm_b = tf.Variable(tf.zeros([vocab_size]), name=\"sm_b\")\n",
    "\n",
    "examples = tf.constant([2,1, 3, 4, 0, 5])\n",
    "labels = tf.constant([8, 9, 10, 11, 12, 13])\n",
    "\n",
    "labels_matrix = tf.reshape(\n",
    "        tf.cast(labels,\n",
    "                dtype=tf.int64),\n",
    "        [batch_size, 1])\n",
    "\n",
    "num_samples = 2\n",
    "sampled_ids, a, b = (tf.nn.uniform_candidate_sampler(\n",
    "        true_classes=labels_matrix,\n",
    "        num_true=1,\n",
    "        num_sampled=num_samples,\n",
    "        unique=True,\n",
    "        range_max=vocab_size,\n",
    "        seed = 100))\n",
    "\n",
    "# Embeddings for examples: [batch_size, emb_dim]\n",
    "example_emb = tf.nn.embedding_lookup(emb, examples)\n",
    "\n",
    "# Weights for labels: [batch_size, emb_dim]\n",
    "true_w = tf.nn.embedding_lookup(sm_w_t, labels)\n",
    "# Biases for labels: [batch_size, 1]\n",
    "true_b = tf.nn.embedding_lookup(sm_b, labels)\n",
    "\n",
    "# Weights for sampled ids: [num_sampled, emb_dim]\n",
    "sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n",
    "# Biases for sampled ids: [num_sampled, 1]\n",
    "sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n",
    "\n",
    "# True logits: [batch_size, 1]\n",
    "true_logits = tf.reduce_sum(tf.mul(example_emb, true_w), 1) + true_b\n",
    "#true_logits = tf.matmul(example_emb, true_w, transpose_b=True) + true_b\n",
    "#true_logits = tf.matmul(example_emb, true_w)\n",
    "# Sampled logits: [batch_size, num_sampled]\n",
    "# We replicate sampled noise lables for all examples in the batch\n",
    "# using the matmul.\n",
    "sampled_b_vec = tf.reshape(sampled_b, [num_samples])\n",
    "sampled_logits = tf.matmul(example_emb,\n",
    "                               sampled_w,\n",
    "                               transpose_b=True) + sampled_b_vec\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "print sampled_ids.eval()\n",
    "print a.eval()\n",
    "print b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1, 1, 1]\n",
      "    #         [1, 1, 1]]\n",
      "    tf.reduce_sum(x) ==> 6\n",
      "    tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
      "    tf.reduce_sum(x, 1) ==> [3, 3]\n",
      "    tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
      "    tf.reduce_sum(x, [0, 1]) ==> 6\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, examples, labels):\n",
    "    \"\"\"Build the graph for the forward pass.\"\"\"\n",
    "    opts = self._options\n",
    "\n",
    "    # Declare all variables we need.\n",
    "    # Embedding: [vocab_size, emb_dim]\n",
    "    init_width = 0.5 / opts.emb_dim\n",
    "    emb = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [opts.vocab_size, opts.emb_dim], -init_width, init_width),\n",
    "        name=\"emb\")\n",
    "    self._emb = emb\n",
    "\n",
    "    # Softmax weight: [vocab_size, emb_dim]. Transposed.\n",
    "    sm_w_t = tf.Variable(\n",
    "        tf.zeros([opts.vocab_size, opts.emb_dim]),\n",
    "        name=\"sm_w_t\")\n",
    "\n",
    "    # Softmax bias: [emb_dim].\n",
    "    sm_b = tf.Variable(tf.zeros([opts.vocab_size]), name=\"sm_b\")\n",
    "\n",
    "    # Global step: scalar, i.e., shape [].\n",
    "    self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "    # Nodes to compute the nce loss w/ candidate sampling.\n",
    "    labels_matrix = tf.reshape(\n",
    "        tf.cast(labels,\n",
    "                dtype=tf.int64),\n",
    "        [opts.batch_size, 1])\n",
    "\n",
    "    # Negative sampling.\n",
    "    sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "        true_classes=labels_matrix,\n",
    "        num_true=1,\n",
    "        num_sampled=opts.num_samples,\n",
    "        unique=True,\n",
    "        range_max=opts.vocab_size,\n",
    "        distortion=0.75,\n",
    "        unigrams=opts.vocab_counts.tolist()))\n",
    "\n",
    "    # Embeddings for examples: [batch_size, emb_dim]\n",
    "    example_emb = tf.nn.embedding_lookup(emb, examples)\n",
    "\n",
    "    # Weights for labels: [batch_size, emb_dim]\n",
    "    true_w = tf.nn.embedding_lookup(sm_w_t, labels)\n",
    "    # Biases for labels: [batch_size, 1]\n",
    "    true_b = tf.nn.embedding_lookup(sm_b, labels)\n",
    "\n",
    "    # Weights for sampled ids: [num_sampled, emb_dim]\n",
    "    sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n",
    "    # Biases for sampled ids: [num_sampled, 1]\n",
    "    sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n",
    "\n",
    "    # True logits: [batch_size, 1]\n",
    "    true_logits = tf.reduce_sum(tf.mul(example_emb, true_w), 1) + true_b\n",
    "\n",
    "    # Sampled logits: [batch_size, num_sampled]\n",
    "    # We replicate sampled noise lables for all examples in the batch\n",
    "    # using the matmul.\n",
    "    sampled_b_vec = tf.reshape(sampled_b, [opts.num_samples])\n",
    "    sampled_logits = tf.matmul(example_emb,\n",
    "                               sampled_w,\n",
    "                               transpose_b=True) + sampled_b_vec\n",
    "    return true_logits, sampled_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
