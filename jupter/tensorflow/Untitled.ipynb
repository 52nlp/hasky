{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2a4493bec97c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2a4493bec97c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    http://172.20.72.11:6006\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://172.20.72.11:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sigmoid_cross_entropy_with_logits in module tensorflow.python.ops.nn:\n",
      "\n",
      "sigmoid_cross_entropy_with_logits(logits, targets, name=None)\n",
      "    Computes sigmoid cross entropy given `logits`.\n",
      "    \n",
      "    Measures the probability error in discrete classification tasks in which each\n",
      "    class is independent and not mutually exclusive.  For instance, one could\n",
      "    perform multilabel classification where a picture can contain both an elephant\n",
      "    and a dog at the same time.\n",
      "    \n",
      "    For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n",
      "    \n",
      "          z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
      "        = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n",
      "        = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n",
      "        = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n",
      "        = (1 - z) * x + log(1 + exp(-x))\n",
      "        = x - x * z + log(1 + exp(-x))\n",
      "    \n",
      "    For x < 0, to avoid overflow in exp(-x), we reformulate the above\n",
      "    \n",
      "          x - x * z + log(1 + exp(-x))\n",
      "        = log(exp(x)) - x * z + log(1 + exp(-x))\n",
      "        = - x * z + log(1 + exp(x))\n",
      "    \n",
      "    Hence, to ensure stability and avoid overflow, the implementation uses this\n",
      "    equivalent formulation\n",
      "    \n",
      "        max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
      "    \n",
      "    `logits` and `targets` must have the same type and shape.\n",
      "    \n",
      "    Args:\n",
      "      logits: A `Tensor` of type `float32` or `float64`.\n",
      "      targets: A `Tensor` of the same type and shape as `logits`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of the same shape as `logits` with the componentwise\n",
      "      logistic losses.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `logits` and `targets` do not have the same shape.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.sigmoid_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_softmax_cross_entropy_with_logits in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)\n",
      "    Computes sparse softmax cross entropy between `logits` and `labels`.\n",
      "    \n",
      "    Measures the probability error in discrete classification tasks in which the\n",
      "    classes are mutually exclusive (each entry is in exactly one class).  For\n",
      "    example, each CIFAR-10 image is labeled with one and only one label: an image\n",
      "    can be a dog or a truck, but not both.\n",
      "    \n",
      "    **NOTE:**  For this operation, the probability of a given label is considered\n",
      "    exclusive.  That is, soft classes are not allowed, and the `labels` vector\n",
      "    must provide a single specific index for the true class for each row of\n",
      "    `logits` (each minibatch entry).  For soft softmax classification with\n",
      "    a probability distribution for each entry, see\n",
      "    `softmax_cross_entropy_with_logits`.\n",
      "    \n",
      "    **WARNING:** This op expects unscaled logits, since it performs a softmax\n",
      "    on `logits` internally for efficiency.  Do not call this op with the\n",
      "    output of `softmax`, as it will produce incorrect results.\n",
      "    \n",
      "    `logits` must have the shape `[batch_size, num_classes]`\n",
      "    and dtype `float32` or `float64`.\n",
      "    \n",
      "    `labels` must have the shape `[batch_size]` and dtype `int32` or `int64`.\n",
      "    \n",
      "    Args:\n",
      "      logits: Unscaled log probabilities.\n",
      "      labels: Each entry `labels[i]` must be an index in `[0, num_classes)`. Other\n",
      "        values will result in a loss of 0, but incorrect gradient computations.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the\n",
      "      softmax cross entropy loss.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.sparse_softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function in_top_k in module tensorflow.python.ops.gen_nn_ops:\n",
      "\n",
      "in_top_k(predictions, targets, k, name=None)\n",
      "    Says whether the targets are in the top `K` predictions.\n",
      "    \n",
      "    This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\n",
      "    prediction for the target class is among the top `k` predictions among\n",
      "    all predictions for example `i`. Note that the behavior of `InTopK` differs\n",
      "    from the `TopK` op in its handling of ties; if multiple classes have the\n",
      "    same prediction value and straddle the top-`k` boundary, all of those\n",
      "    classes are considered to be in the top `k`.\n",
      "    \n",
      "    More formally, let\n",
      "    \n",
      "      \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n",
      "      \\\\(targets_i\\\\) be the target class for example `i`,\n",
      "      \\\\(out_i\\\\) be the output for example `i`,\n",
      "    \n",
      "    $$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$\n",
      "    \n",
      "    Args:\n",
      "      predictions: A `Tensor` of type `float32`.\n",
      "        A `batch_size` x `classes` tensor.\n",
      "      targets: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        A `batch_size` vector of class ids.\n",
      "      k: An `int`. Number of top elements to look at for computing precision.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `bool`. Computed Precision at `k` as a `bool Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.in_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_pool in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
      "    Performs the max pooling on the input.\n",
      "    \n",
      "    Args:\n",
      "      value: A 4-D `Tensor` with shape `[batch, height, width, channels]` and\n",
      "        type `tf.float32`.\n",
      "      ksize: A list of ints that has length >= 4.  The size of the window for\n",
      "        each dimension of the input tensor.\n",
      "      strides: A list of ints that has length >= 4.  The stride of the sliding\n",
      "        window for each dimension of the input tensor.\n",
      "      padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm.\n",
      "      data_format: A string. 'NHWC' and 'NCHW' are supported.\n",
      "      name: Optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with type `tf.float32`.  The max pooled output tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function conv2d in module tensorflow.python.ops.gen_nn_ops:\n",
      "\n",
      "conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
      "    Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n",
      "    \n",
      "    Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
      "    and a filter / kernel tensor of shape\n",
      "    `[filter_height, filter_width, in_channels, out_channels]`, this op\n",
      "    performs the following:\n",
      "    \n",
      "    1. Flattens the filter to a 2-D matrix with shape\n",
      "       `[filter_height * filter_width * in_channels, output_channels]`.\n",
      "    2. Extracts image patches from the input tensor to form a *virtual*\n",
      "       tensor of shape `[batch, out_height, out_width,\n",
      "       filter_height * filter_width * in_channels]`.\n",
      "    3. For each patch, right-multiplies the filter matrix and the image patch\n",
      "       vector.\n",
      "    \n",
      "    In detail, with the default NHWC format,\n",
      "    \n",
      "        output[b, i, j, k] =\n",
      "            sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n",
      "                            filter[di, dj, q, k]\n",
      "    \n",
      "    Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\n",
      "    horizontal and vertices strides, `strides = [1, stride, stride, 1]`.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n",
      "      filter: A `Tensor`. Must have the same type as `input`.\n",
      "      strides: A list of `ints`.\n",
      "        1-D of length 4.  The stride of the sliding window for each dimension\n",
      "        of `input`. Must be in the same order as the dimension specified with format.\n",
      "      padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "        The type of padding algorithm to use.\n",
      "      use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.\n",
      "      data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "        Specify the data format of the input and output data. With the\n",
      "        default format \"NHWC\", the data is stored in the order of:\n",
      "            [batch, in_height, in_width, in_channels].\n",
      "        Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "            [batch, in_channels, in_height, in_width].\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "[[[[ 5.]]]]\n",
      "[[[[ 5.]\n",
      "   [ 6.]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x51dde50>> ignored\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "x = tf.reshape(x, [1, 2, 3, 1])  # give a shape accepted by tf.nn.max_pool\n",
    "\n",
    "valid_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "same_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "print valid_pad.get_shape() == [1, 1, 1, 1]  # valid_pad is [5.]\n",
    "print same_pad.get_shape() == [1, 1, 2, 1]   # same_pad is  [5., 6.]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print valid_pad.eval()\n",
    "print same_pad.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Features in module tensorflow.core.example.feature_pb2:\n",
      "\n",
      "class Features(google.protobuf.message.Message)\n",
      " |  Method resolution order:\n",
      " |      Features\n",
      " |      google.protobuf.message.Message\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  ByteSize(self)\n",
      " |  \n",
      " |  Clear(self)\n",
      " |  \n",
      " |  ClearField(self, field_name)\n",
      " |  \n",
      " |  FindInitializationErrors(self)\n",
      " |      Finds required fields which are not initialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.  Each string is a path to an uninitialized field from\n",
      " |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      " |  \n",
      " |  HasField(self, field_name)\n",
      " |  \n",
      " |  IsInitialized(self, errors=None)\n",
      " |      Checks if all required fields of a message are set.\n",
      " |      \n",
      " |      Args:\n",
      " |        errors:  A list which, if provided, will be populated with the field\n",
      " |                 paths of all missing required fields.\n",
      " |      \n",
      " |      Returns:\n",
      " |        True iff the specified message has all required fields set.\n",
      " |  \n",
      " |  ListFields(self)\n",
      " |  \n",
      " |  MergeFrom(self, msg)\n",
      " |  \n",
      " |  MergeFromString(self, serialized)\n",
      " |  \n",
      " |  SerializePartialToString(self)\n",
      " |  \n",
      " |  SerializeToString(self)\n",
      " |  \n",
      " |  SetInParent = Modified(self)\n",
      " |      Sets the _cached_byte_size_dirty bit to true,\n",
      " |      and propagates this to our listener iff this was a state change.\n",
      " |  \n",
      " |  WhichOneof(self, oneof_name)\n",
      " |      Returns the name of the currently set field inside a oneof, or None.\n",
      " |  \n",
      " |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      " |  \n",
      " |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      " |  \n",
      " |  _Modified = Modified(self)\n",
      " |      Sets the _cached_byte_size_dirty bit to true,\n",
      " |      and propagates this to our listener iff this was a state change.\n",
      " |  \n",
      " |  _SetListener = SetListener(self, listener)\n",
      " |  \n",
      " |  _UpdateOneofState(self, field)\n",
      " |      Sets field as the active field in its containing oneof.\n",
      " |      \n",
      " |      Will also delete currently active field in the oneof, if it is different\n",
      " |      from the argument. Does not mark the message as modified.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__ = init(self, **kwargs)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  FromString(s)\n",
      " |  \n",
      " |  RegisterExtension(extension_handle)\n",
      " |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  _cached_byte_size\n",
      " |  \n",
      " |  _cached_byte_size_dirty\n",
      " |  \n",
      " |  _fields\n",
      " |  \n",
      " |  _is_present_in_parent\n",
      " |  \n",
      " |  _listener\n",
      " |  \n",
      " |  _listener_for_children\n",
      " |  \n",
      " |  _oneofs\n",
      " |  \n",
      " |  _unknown_fields\n",
      " |  \n",
      " |  feature\n",
      " |      Magic attribute generated for \"feature\" proto field.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      " |  \n",
      " |  FEATURE_FIELD_NUMBER = 1\n",
      " |  \n",
      " |  FeatureEntry = <class 'tensorflow.core.example.feature_pb2.FeatureEntr...\n",
      " |  \n",
      " |  \n",
      " |  _decoders_by_tag = {'\\n': (<function DecodeMap>, None)}\n",
      " |  \n",
      " |  _extensions_by_name = {}\n",
      " |  \n",
      " |  _extensions_by_number = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.message.Message:\n",
      " |  \n",
      " |  ClearExtension(self, extension_handle)\n",
      " |  \n",
      " |  CopyFrom(self, other_msg)\n",
      " |      Copies the content of the specified message into the current message.\n",
      " |      \n",
      " |      The method clears the current message and then merges the specified\n",
      " |      message using MergeFrom.\n",
      " |      \n",
      " |      Args:\n",
      " |        other_msg: Message to copy into the current one.\n",
      " |  \n",
      " |  HasExtension(self, extension_handle)\n",
      " |  \n",
      " |  ParseFromString(self, serialized)\n",
      " |      Parse serialized protocol buffer data into this message.\n",
      " |      \n",
      " |      Like MergeFromString(), except we clear the object first and\n",
      " |      do not return the value that MergeFromString returns.\n",
      " |  \n",
      " |  __deepcopy__(self, memo=None)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Support the pickle protocol.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |  \n",
      " |  __ne__(self, other_msg)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |      Support the pickle protocol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Example in module tensorflow.core.example.example_pb2:\n",
      "\n",
      "class Example(google.protobuf.message.Message)\n",
      " |  Method resolution order:\n",
      " |      Example\n",
      " |      google.protobuf.message.Message\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  ByteSize(self)\n",
      " |  \n",
      " |  Clear(self)\n",
      " |  \n",
      " |  ClearField(self, field_name)\n",
      " |  \n",
      " |  FindInitializationErrors(self)\n",
      " |      Finds required fields which are not initialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.  Each string is a path to an uninitialized field from\n",
      " |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      " |  \n",
      " |  HasField(self, field_name)\n",
      " |  \n",
      " |  IsInitialized(self, errors=None)\n",
      " |      Checks if all required fields of a message are set.\n",
      " |      \n",
      " |      Args:\n",
      " |        errors:  A list which, if provided, will be populated with the field\n",
      " |                 paths of all missing required fields.\n",
      " |      \n",
      " |      Returns:\n",
      " |        True iff the specified message has all required fields set.\n",
      " |  \n",
      " |  ListFields(self)\n",
      " |  \n",
      " |  MergeFrom(self, msg)\n",
      " |  \n",
      " |  MergeFromString(self, serialized)\n",
      " |  \n",
      " |  SerializePartialToString(self)\n",
      " |  \n",
      " |  SerializeToString(self)\n",
      " |  \n",
      " |  SetInParent = Modified(self)\n",
      " |      Sets the _cached_byte_size_dirty bit to true,\n",
      " |      and propagates this to our listener iff this was a state change.\n",
      " |  \n",
      " |  WhichOneof(self, oneof_name)\n",
      " |      Returns the name of the currently set field inside a oneof, or None.\n",
      " |  \n",
      " |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      " |  \n",
      " |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      " |  \n",
      " |  _Modified = Modified(self)\n",
      " |      Sets the _cached_byte_size_dirty bit to true,\n",
      " |      and propagates this to our listener iff this was a state change.\n",
      " |  \n",
      " |  _SetListener = SetListener(self, listener)\n",
      " |  \n",
      " |  _UpdateOneofState(self, field)\n",
      " |      Sets field as the active field in its containing oneof.\n",
      " |      \n",
      " |      Will also delete currently active field in the oneof, if it is different\n",
      " |      from the argument. Does not mark the message as modified.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__ = init(self, **kwargs)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  FromString(s)\n",
      " |  \n",
      " |  RegisterExtension(extension_handle)\n",
      " |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  _cached_byte_size\n",
      " |  \n",
      " |  _cached_byte_size_dirty\n",
      " |  \n",
      " |  _fields\n",
      " |  \n",
      " |  _is_present_in_parent\n",
      " |  \n",
      " |  _listener\n",
      " |  \n",
      " |  _listener_for_children\n",
      " |  \n",
      " |  _oneofs\n",
      " |  \n",
      " |  _unknown_fields\n",
      " |  \n",
      " |  features\n",
      " |      Magic attribute generated for \"features\" proto field.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      " |  \n",
      " |  FEATURES_FIELD_NUMBER = 1\n",
      " |  \n",
      " |  _decoders_by_tag = {'\\n': (<function DecodeField>, None)}\n",
      " |  \n",
      " |  _extensions_by_name = {}\n",
      " |  \n",
      " |  _extensions_by_number = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.message.Message:\n",
      " |  \n",
      " |  ClearExtension(self, extension_handle)\n",
      " |  \n",
      " |  CopyFrom(self, other_msg)\n",
      " |      Copies the content of the specified message into the current message.\n",
      " |      \n",
      " |      The method clears the current message and then merges the specified\n",
      " |      message using MergeFrom.\n",
      " |      \n",
      " |      Args:\n",
      " |        other_msg: Message to copy into the current one.\n",
      " |  \n",
      " |  HasExtension(self, extension_handle)\n",
      " |  \n",
      " |  ParseFromString(self, serialized)\n",
      " |      Parse serialized protocol buffer data into this message.\n",
      " |      \n",
      " |      Like MergeFromString(), except we clear the object first and\n",
      " |      do not return the value that MergeFromString returns.\n",
      " |  \n",
      " |  __deepcopy__(self, memo=None)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Support the pickle protocol.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |  \n",
      " |  __ne__(self, other_msg)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |      Support the pickle protocol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function string_input_producer in module tensorflow.python.training.input:\n",
      "\n",
      "string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)\n",
      "    Output strings (e.g. filenames) to a queue for an input pipeline.\n",
      "    \n",
      "    Args:\n",
      "      string_tensor: A 1-D string tensor with the strings to produce.\n",
      "      num_epochs: An integer (optional). If specified, `string_input_producer`\n",
      "        produces each string from `string_tensor` `num_epochs` times before\n",
      "        generating an `OutOfRange` error. If not specified,\n",
      "        `string_input_producer` can cycle through the strings in `string_tensor`\n",
      "        an unlimited number of times.\n",
      "      shuffle: Boolean. If true, the strings are randomly shuffled within each\n",
      "        epoch.\n",
      "      seed: An integer (optional). Seed used if shuffle == True.\n",
      "      capacity: An integer. Sets the queue capacity.\n",
      "      shared_name: (optional). If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: A name for the operations (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A queue with the output strings.  A `QueueRunner` for the Queue\n",
      "      is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the string_tensor is a null Python list.  At runtime,\n",
      "      will fail with an assertion if string_tensor becomes a null tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( tf.train.string_input_producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function shuffle_batch in module tensorflow.python.training.input:\n",
      "\n",
      "shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, shared_name=None, name=None)\n",
      "    Creates batches by randomly shuffling tensors.\n",
      "    \n",
      "    This function adds the following to the current `Graph`:\n",
      "    \n",
      "    * A shuffling queue into which tensors from `tensor_list` are enqueued.\n",
      "    * A `dequeue_many` operation to create batches from the queue.\n",
      "    * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n",
      "      from `tensor_list`.\n",
      "    \n",
      "    If `enqueue_many` is `False`, `tensor_list` is assumed to represent a\n",
      "    single example.  An input tensor with shape `[x, y, z]` will be output\n",
      "    as a tensor with shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    If `enqueue_many` is `True`, `tensor_list` is assumed to represent a\n",
      "    batch of examples, where the first dimension is indexed by example,\n",
      "    and all members of `tensor_list` should have the same size in the\n",
      "    first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
      "    output will have shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    The `capacity` argument controls the how long the prefetching is allowed to\n",
      "    grow the queues.\n",
      "    \n",
      "    The returned operation is a dequeue operation and will throw\n",
      "    `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "    operation is feeding another input queue, its queue runner will catch\n",
      "    this exception, however, if this operation is used in your main thread\n",
      "    you are responsible for catching this yourself.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # Creates batches of 32 images and 32 labels.\n",
      "    image_batch, label_batch = tf.train.shuffle_batch(\n",
      "          [single_image, single_label],\n",
      "          batch_size=32,\n",
      "          num_threads=4,\n",
      "          capacity=50000,\n",
      "          min_after_dequeue=10000)\n",
      "    ```\n",
      "    \n",
      "    *N.B.:* You must ensure that either (i) the `shapes` argument is\n",
      "    passed, or (ii) all of the tensors in `tensor_list` must have\n",
      "    fully-defined shapes. `ValueError` will be raised if neither of\n",
      "    these conditions holds.\n",
      "    \n",
      "    Args:\n",
      "      tensor_list: The list of tensors to enqueue.\n",
      "      batch_size: The new batch size pulled from the queue.\n",
      "      capacity: An integer. The maximum number of elements in the queue.\n",
      "      min_after_dequeue: Minimum number elements in the queue after a\n",
      "        dequeue, used to ensure a level of mixing of elements.\n",
      "      num_threads: The number of threads enqueuing `tensor_list`.\n",
      "      seed: Seed for the random shuffling within the queue.\n",
      "      enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "      shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "        inferred shapes for `tensor_list`.\n",
      "      shared_name: (Optional) If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: (Optional) A name for the operations.\n",
      "    \n",
      "    Returns:\n",
      "      A list of tensors with the same number and types as `tensor_list`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the `shapes` are not specified, and cannot be\n",
      "        inferred from the elements of `tensor_list`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.shuffle_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function start_queue_runners in module tensorflow.python.training.queue_runner:\n",
      "\n",
      "start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')\n",
      "    Starts all queue runners collected in the graph.\n",
      "    \n",
      "    This is a companion method to `add_queue_runner()`.  It just starts\n",
      "    threads for all queue runners collected in the graph.  It returns\n",
      "    the list of all threads.\n",
      "    \n",
      "    Args:\n",
      "      sess: `Session` used to run the queue ops.  Defaults to the\n",
      "        default session.\n",
      "      coord: Optional `Coordinator` for coordinating the started threads.\n",
      "      daemon: Whether the threads should be marked as `daemons`, meaning\n",
      "        they don't block program exit.\n",
      "      start: Set to `False` to only create the threads, not start them.\n",
      "      collection: A `GraphKey` specifying the graph collection to\n",
      "        get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n",
      "    \n",
      "    Returns:\n",
      "      A list of threads.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.start_queue_runners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "q = tf.FIFOQueue(3, 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = q.enqueue_many(([0.,0.,0.],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = q.dequeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_inc = q.enqueue([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_inc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_inc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_inc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_inc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gather in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "gather(params, indices, validate_indices=None, name=None)\n",
      "    Gather slices from `params` according to `indices`.\n",
      "    \n",
      "    `indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\n",
      "    Produces an output tensor with shape `indices.shape + params.shape[1:]` where:\n",
      "    \n",
      "        # Scalar indices\n",
      "        output[:, ..., :] = params[indices, :, ... :]\n",
      "    \n",
      "        # Vector indices\n",
      "        output[i, :, ..., :] = params[indices[i], :, ... :]\n",
      "    \n",
      "        # Higher rank indices\n",
      "        output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n",
      "    \n",
      "    If `indices` is a permutation and `len(indices) == params.shape[0]` then\n",
      "    this operation will permute `params` accordingly.\n",
      "    \n",
      "    <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
      "    <img style=\"width:100%\" src=\"../../images/Gather.png\" alt>\n",
      "    </div>\n",
      "    \n",
      "    Args:\n",
      "      params: A `Tensor`.\n",
      "      indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      validate_indices: An optional `bool`. Defaults to `True`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `params`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help(tf.gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
