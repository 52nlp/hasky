{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([ 0.7933414], dtype=float32), array([-0.14793825], dtype=float32))\n",
      "(20, array([ 0.31563386], dtype=float32), array([ 0.17733581], dtype=float32))\n",
      "(40, array([ 0.16490985], dtype=float32), array([ 0.26307577], dtype=float32))\n",
      "(60, array([ 0.1195391], dtype=float32), array([ 0.28888512], dtype=float32))\n",
      "(80, array([ 0.10588165], dtype=float32), array([ 0.29665422], dtype=float32))\n",
      "(100, array([ 0.10177048], dtype=float32), array([ 0.29899287], dtype=float32))\n",
      "(120, array([ 0.10053294], dtype=float32), array([ 0.29969686], dtype=float32))\n",
      "(140, array([ 0.10016042], dtype=float32), array([ 0.29990876], dtype=float32))\n",
      "(160, array([ 0.10004829], dtype=float32), array([ 0.29997253], dtype=float32))\n",
      "(180, array([ 0.10001455], dtype=float32), array([ 0.29999173], dtype=float32))\n",
      "(200, array([ 0.1000044], dtype=float32), array([ 0.29999751], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n",
    "\n",
    "# Learns best fit is W: [0.1], b: [0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "# Launch the default graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of three ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> [[ 12.]]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "# We can just use 'c.eval()' without passing 'sess'\n",
    "print(c.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using eval(): No default session is registered. Use `with sess.as_default()` or pass an explicit session to eval(session=sess)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-13ef8b565346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# We can just use 'c.eval()' without passing 'sess'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \"\"\"\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3360\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3362\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using eval(): No default \"\n\u001b[0m\u001b[0;32m   3363\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3364\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using eval(): No default session is registered. Use `with sess.as_default()` or pass an explicit session to eval(session=sess)"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "# We can just use 'c.eval()' without passing 'sess'\n",
    "print(c.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(range(12), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.constant(0.9)\n",
    "h_fc1_drop = tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   1.11111116,   2.22222233,   3.33333349,\n",
       "         4.44444466,   5.55555582,   6.66666698,   7.77777815,\n",
       "         8.88888931,  10.        ,  11.11111164,  12.22222328], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc1_drop.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'iris_training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c98f8a8d2837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Load datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIRIS_TRAINING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIRIS_TEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.pyc\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(filename, target_dtype, target_column, has_header)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/platform/gfile.pyc\u001b[0m in \u001b[0;36mOpen\u001b[1;34m(name, mode)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mthreadsafe\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m   \"\"\"\n\u001b[1;32m--> 452\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/platform/gfile.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Pythonlocker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/platform/gfile.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, locker)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'iris_training.csv'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "\n",
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING, target_dtype=np.int)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST, target_dtype=np.int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = training_set.data, test_set.data, \\\n",
    "  training_set.target, test_set.target\n",
    "\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\n",
    "\n",
    "# Fit model.\n",
    "classifier.fit(x=x_train, y=y_train, steps=200)\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x=x_test, y=y_test)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))\n",
    "\n",
    "# Classify two new flower samples.\n",
    "new_samples = np.array(\n",
    "    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n",
    "y = classifier.predict(new_samples)\n",
    "print ('Predictions: {}'.format(str(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         0.2        -0.1         1.         -0.5         0.30000001]\n",
      "[[ 0.5         0.2        -0.1       ]\n",
      " [ 1.         -0.5         0.30000001]]\n",
      "[[ 0.5]\n",
      " [ 1. ]]\n",
      "[[ 0.2        -0.1       ]\n",
      " [-0.5         0.30000001]]\n",
      "[[ 0.30000001  0.60000002]\n",
      " [ 1.5         0.69999999]]\n",
      "[[ 0.69999999  0.40000001]\n",
      " [-0.5         0.30000001]]\n",
      "[[ 0.69999999  0.40000001]\n",
      " [ 0.          0.30000001]]\n",
      "1.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "B = 2\n",
    "N = 2\n",
    "scores = tf.constant([0.5, 0.2, -0.1, 1., -0.5, 0.3])  # shape B * (N+1)\n",
    "\n",
    "print scores.eval()\n",
    "\n",
    "scores = tf.reshape(scores, [B, N+1])\n",
    "print scores.eval()\n",
    "\n",
    "scores_pos = tf.slice(scores, [0, 0], [B, 1])\n",
    "print scores_pos.eval()\n",
    "scores_neg = tf.slice(scores, [0, 1], [B, N])\n",
    "print scores_neg.eval()\n",
    "print (scores_pos - scores_neg).eval()\n",
    "print (1. - scores_pos + scores_neg).eval()\n",
    "loss_matrix = tf.maximum(0., 1. - scores_pos + scores_neg)  # we could also use tf.nn.relu here\n",
    "print loss_matrix.eval()\n",
    "loss = tf.reduce_sum(loss_matrix)\n",
    "print loss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5],\n",
       "       [ 1. ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pos.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2       , -0.1       ],\n",
       "       [-0.5       ,  0.30000001]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function slice in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "slice(input_, begin, size, name=None)\n",
      "    Extracts a slice from a tensor.\n",
      "    \n",
      "    This operation extracts a slice of size `size` from a tensor `input` starting\n",
      "    at the location specified by `begin`. The slice `size` is represented as a\n",
      "    tensor shape, where `size[i]` is the number of elements of the 'i'th dimension\n",
      "    of `input` that you want to slice. The starting location (`begin`) for the\n",
      "    slice is represented as an offset in each dimension of `input`. In other\n",
      "    words, `begin[i]` is the offset into the 'i'th dimension of `input` that you\n",
      "    want to slice from.\n",
      "    \n",
      "    `begin` is zero-based; `size` is one-based. If `size[i]` is -1,\n",
      "    all remaining elements in dimension i are included in the\n",
      "    slice. In other words, this is equivalent to setting:\n",
      "    \n",
      "    `size[i] = input.dim_size(i) - begin[i]`\n",
      "    \n",
      "    This operation requires that:\n",
      "    \n",
      "    `0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n]`\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    # 'input' is [[[1, 1, 1], [2, 2, 2]],\n",
      "    #             [[3, 3, 3], [4, 4, 4]],\n",
      "    #             [[5, 5, 5], [6, 6, 6]]]\n",
      "    tf.slice(input, [1, 0, 0], [1, 1, 3]) ==> [[[3, 3, 3]]]\n",
      "    tf.slice(input, [1, 0, 0], [1, 2, 3]) ==> [[[3, 3, 3],\n",
      "                                                [4, 4, 4]]]\n",
      "    tf.slice(input, [1, 0, 0], [2, 1, 3]) ==> [[[3, 3, 3]],\n",
      "                                               [[5, 5, 5]]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_: A `Tensor`.\n",
      "      begin: An `int32` or `int64` `Tensor`.\n",
      "      size: An `int32` or `int64` `Tensor`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 1, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 1, 32], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1., 2., 3., 4.],\n",
    "                 [4., 5., 6., 7.],\n",
    "                [8., 9., 10., 11.],\n",
    "                [12.,13.,14.,15.]])\n",
    "\n",
    "x = tf.reshape(x, [1, 4, 4, 1])  # give a shape accepted by tf.nn.max_pool\n",
    "#result1 = my_image_filter(x)\n",
    "#result2 = my_image_filter(x)\n",
    "# Raises ValueError(... conv1/weights already exists ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable image_filters/conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-39-da151707662a>\", line 4, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-39-da151707662a>\", line 14, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n  File \"<ipython-input-43-fa878f4f7f27>\", line 8, in <module>\n    result1 = my_image_filter(x)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-f9986da23c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# give a shape accepted by tf.nn.max_pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image_filters\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresult1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#result2 = my_image_filter(y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-da01e87019d7>\u001b[0m in \u001b[0;36mmy_image_filter\u001b[1;34m(input_images)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Variables created here will be named \"conv1/weights\", \"conv1/biases\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mrelu1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"conv2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Variables created here will be named \"conv2/weights\", \"conv2/biases\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-da01e87019d7>\u001b[0m in \u001b[0;36mconv_relu\u001b[1;34m(input, kernel_shape, bias_shape)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Create variable named \"weights\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     weights = tf.get_variable(\"weights\", kernel_shape,\n\u001b[1;32m----> 4\u001b[1;33m         initializer=tf.random_normal_initializer())\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Create variable named \"biases\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     biases = tf.get_variable(\"biases\", bias_shape,\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    684\u001b[0m       \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m       partitioner=partitioner, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    554\u001b[0m           \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m           partitioner=partitioner, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m   def _get_partitioned_variable_list(\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m   def _get_partitioned_variable_list(\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    395\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 397\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    398\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable image_filters/conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-39-da151707662a>\", line 4, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-39-da151707662a>\", line 14, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n  File \"<ipython-input-43-fa878f4f7f27>\", line 8, in <module>\n    result1 = my_image_filter(x)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([[1., 2., 3., 4.],\n",
    "                 [4., 5., 6., 7.],\n",
    "                [8., 9., 10., 11.],\n",
    "                [12.,13.,14.,15.]])\n",
    "\n",
    "y = tf.reshape(x, [1, 4, 4, 1])  # give a shape accepted by tf.nn.max_pool\n",
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter(y)\n",
    "    scope.reuse_variables()\n",
    "    #result2 = my_image_filter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch in module tensorflow.python.training.input:\n",
      "\n",
      "batch(tensor_list, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, shared_name=None, name=None)\n",
      "    Creates batches of tensors in `tensor_list`.\n",
      "    \n",
      "    This function is implemented using a queue. A `QueueRunner` for the\n",
      "    queue is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "    \n",
      "    If `enqueue_many` is `False`, `tensor_list` is assumed to represent a\n",
      "    single example.  An input tensor with shape `[x, y, z]` will be output\n",
      "    as a tensor with shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    If `enqueue_many` is `True`, `tensor_list` is assumed to represent a\n",
      "    batch of examples, where the first dimension is indexed by example,\n",
      "    and all members of `tensor_list` should have the same size in the\n",
      "    first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
      "    output will have shape `[batch_size, x, y, z]`.  The `capacity` argument\n",
      "    controls the how long the prefetching is allowed to grow the queues.\n",
      "    \n",
      "    The returned operation is a dequeue operation and will throw\n",
      "    `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "    operation is feeding another input queue, its queue runner will catch\n",
      "    this exception, however, if this operation is used in your main thread\n",
      "    you are responsible for catching this yourself.\n",
      "    \n",
      "    *N.B.:* If `dynamic_pad` is `False`, you must ensure that either\n",
      "    (i) the `shapes` argument is passed, or (ii) all of the tensors in\n",
      "    `tensor_list` must have fully-defined shapes. `ValueError` will be\n",
      "    raised if neither of these conditions holds.\n",
      "    \n",
      "    If `dynamic_pad` is `True`, it is sufficient that the *rank* of the\n",
      "    tensors is known, but individual dimensions may have shape `None`.\n",
      "    In this case, for each enqueue the dimensions with value `None`\n",
      "    may have a variable length; upon dequeue, the output tensors will be padded\n",
      "    on the right to the maximum shape of the tensors in the current minibatch.\n",
      "    For numbers, this padding takes value 0.  For strings, this padding is\n",
      "    the empty string.  See `PaddingFIFOQueue` for more info.\n",
      "    \n",
      "    Args:\n",
      "      tensor_list: The list of tensors to enqueue.\n",
      "      batch_size: The new batch size pulled from the queue.\n",
      "      num_threads: The number of threads enqueuing `tensor_list`.\n",
      "      capacity: An integer. The maximum number of elements in the queue.\n",
      "      enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "      shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "        inferred shapes for `tensor_list`.\n",
      "      dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n",
      "        The given dimensions are padded upon dequeue so that tensors within a\n",
      "        batch have the same shapes.\n",
      "      shared_name: (optional). If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: (Optional) A name for the operations.\n",
      "    \n",
      "    Returns:\n",
      "      A list of tensors with the same number and types as `tensor_list`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the `shapes` are not specified, and cannot be\n",
      "        inferred from the elements of `tensor_list`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "help(tf.train.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function shuffle_batch in module tensorflow.python.training.input:\n",
      "\n",
      "shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, shared_name=None, name=None)\n",
      "    Creates batches by randomly shuffling tensors.\n",
      "    \n",
      "    This function adds the following to the current `Graph`:\n",
      "    \n",
      "    * A shuffling queue into which tensors from `tensor_list` are enqueued.\n",
      "    * A `dequeue_many` operation to create batches from the queue.\n",
      "    * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n",
      "      from `tensor_list`.\n",
      "    \n",
      "    If `enqueue_many` is `False`, `tensor_list` is assumed to represent a\n",
      "    single example.  An input tensor with shape `[x, y, z]` will be output\n",
      "    as a tensor with shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    If `enqueue_many` is `True`, `tensor_list` is assumed to represent a\n",
      "    batch of examples, where the first dimension is indexed by example,\n",
      "    and all members of `tensor_list` should have the same size in the\n",
      "    first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
      "    output will have shape `[batch_size, x, y, z]`.\n",
      "    \n",
      "    The `capacity` argument controls the how long the prefetching is allowed to\n",
      "    grow the queues.\n",
      "    \n",
      "    The returned operation is a dequeue operation and will throw\n",
      "    `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "    operation is feeding another input queue, its queue runner will catch\n",
      "    this exception, however, if this operation is used in your main thread\n",
      "    you are responsible for catching this yourself.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # Creates batches of 32 images and 32 labels.\n",
      "    image_batch, label_batch = tf.train.shuffle_batch(\n",
      "          [single_image, single_label],\n",
      "          batch_size=32,\n",
      "          num_threads=4,\n",
      "          capacity=50000,\n",
      "          min_after_dequeue=10000)\n",
      "    ```\n",
      "    \n",
      "    *N.B.:* You must ensure that either (i) the `shapes` argument is\n",
      "    passed, or (ii) all of the tensors in `tensor_list` must have\n",
      "    fully-defined shapes. `ValueError` will be raised if neither of\n",
      "    these conditions holds.\n",
      "    \n",
      "    Args:\n",
      "      tensor_list: The list of tensors to enqueue.\n",
      "      batch_size: The new batch size pulled from the queue.\n",
      "      capacity: An integer. The maximum number of elements in the queue.\n",
      "      min_after_dequeue: Minimum number elements in the queue after a\n",
      "        dequeue, used to ensure a level of mixing of elements.\n",
      "      num_threads: The number of threads enqueuing `tensor_list`.\n",
      "      seed: Seed for the random shuffling within the queue.\n",
      "      enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "      shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "        inferred shapes for `tensor_list`.\n",
      "      shared_name: (Optional) If set, this queue will be shared under the given\n",
      "        name across multiple sessions.\n",
      "      name: (Optional) A name for the operations.\n",
      "    \n",
      "    Returns:\n",
      "      A list of tensors with the same number and types as `tensor_list`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the `shapes` are not specified, and cannot be\n",
      "        inferred from the elements of `tensor_list`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.shuffle_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
