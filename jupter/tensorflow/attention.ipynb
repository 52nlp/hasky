{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow_version: 0.12.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# We disable pylint because we need python3 compatibility.\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from six.moves import zip     # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.python import shape\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "# TODO(ebrevdo): Remove once _linear is fully deprecated.\n",
    "linear = rnn_cell._linear  # pylint: disable=protected-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs,\n",
    "                      initial_state,\n",
    "                      attention_states,\n",
    "                      cell,\n",
    "                      output_size=None,\n",
    "                      num_heads=1,\n",
    "                      loop_function=None,\n",
    "                      dtype=None,\n",
    "                      scope=None,\n",
    "                      initial_state_attention=False):\n",
    "  \"\"\"RNN decoder with attention for the sequence-to-sequence model.\n",
    "\n",
    "  In this context \"attention\" means that, during decoding, the RNN can look up\n",
    "  information in the additional tensor attention_states, and it does this by\n",
    "  focusing on a few entries from the tensor. This model has proven to yield\n",
    "  especially good results in a number of sequence-to-sequence tasks. This\n",
    "  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n",
    "  details). It is recommended for complex sequence-to-sequence tasks.\n",
    "\n",
    "  Args:\n",
    "    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "    initial_state: 2D Tensor [batch_size x cell.state_size].\n",
    "    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n",
    "    cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "    output_size: Size of the output vectors; if None, we use cell.output_size.\n",
    "    num_heads: Number of attention heads that read from attention_states.\n",
    "    loop_function: If not None, this function will be applied to i-th output\n",
    "      in order to generate i+1-th input, and decoder_inputs will be ignored,\n",
    "      except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "      but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "      Signature -- loop_function(prev, i) = next\n",
    "        * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "        * i is an integer, the step number (when advanced control is needed),\n",
    "        * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n",
    "    scope: VariableScope for the created subgraph; default: \"attention_decoder\".\n",
    "    initial_state_attention: If False (default), initial attentions are zero.\n",
    "      If True, initialize the attentions from the initial state and attention\n",
    "      states -- useful when we wish to resume decoding from a previously\n",
    "      stored decoder state and attention states.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the form (outputs, state), where:\n",
    "      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n",
    "        shape [batch_size x output_size]. These represent the generated outputs.\n",
    "        Output i is computed from input i (which is either the i-th element\n",
    "        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n",
    "        First, we run the cell on a combination of the input and previous\n",
    "        attention masks:\n",
    "          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n",
    "        Then, we calculate new attention masks:\n",
    "          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n",
    "        and then we calculate the output:\n",
    "          output = linear(cell_output, new_attn).\n",
    "      state: The state of each decoder cell the final time-step.\n",
    "        It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "\n",
    "  Raises:\n",
    "    ValueError: when num_heads is not positive, there are no inputs, shapes\n",
    "      of attention_states are not set, or input size cannot be inferred\n",
    "      from the input.\n",
    "  \"\"\"\n",
    "  if not decoder_inputs:\n",
    "    raise ValueError(\"Must provide at least 1 input to attention decoder.\")\n",
    "  if num_heads < 1:\n",
    "    raise ValueError(\"With less than 1 heads, use a non-attention decoder.\")\n",
    "  if attention_states.get_shape()[2].value is None:\n",
    "    raise ValueError(\"Shape[2] of attention_states must be known: %s\"\n",
    "                     % attention_states.get_shape())\n",
    "  if output_size is None:\n",
    "    output_size = cell.output_size\n",
    "\n",
    "  with variable_scope.variable_scope(\n",
    "      scope or \"attention_decoder\", dtype=dtype) as scope:\n",
    "    print('scope', scope, scope.name)\n",
    "    x = melt.get_weights('abc', [1,3])\n",
    "    print(x, x.name)\n",
    "    dtype = scope.dtype\n",
    "\n",
    "    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "    attn_length = attention_states.get_shape()[1].value\n",
    "    if attn_length is None:\n",
    "      attn_length = shape(attention_states)[1]\n",
    "    attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "    hidden = array_ops.reshape(\n",
    "        attention_states, [-1, attn_length, 1, attn_size])\n",
    "    hidden_features = []\n",
    "    v = []\n",
    "    attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "    for a in xrange(num_heads):\n",
    "      k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "                                      [1, 1, attn_size, attention_vec_size])\n",
    "      print('k', k, k.name)\n",
    "      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "      v.append(\n",
    "          variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))\n",
    "\n",
    "    state = initial_state\n",
    "    \n",
    "    print('state.name', state[0].name)\n",
    "\n",
    "    def attention(query):\n",
    "      \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "      ds = []  # Results of attention reads will be stored here.\n",
    "      if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n",
    "        query_list = nest.flatten(query)\n",
    "        print('query_list', query_list)\n",
    "        for q in query_list:  # Check that ndims == 2 if specified.\n",
    "          ndims = q.get_shape().ndims\n",
    "          if ndims:\n",
    "            assert ndims == 2\n",
    "        query = array_ops.concat(1, query_list)\n",
    "        print('query', query)\n",
    "      for a in xrange(num_heads):\n",
    "        with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "          y = linear(query, attention_vec_size, True)\n",
    "          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "          print('y', y)\n",
    "          print('hidden_features[0]', hidden_features[0])\n",
    "          z = hidden_features[0] + y\n",
    "          print('z',  z)\n",
    "          # Attention mask is a softmax of v^T * tanh(...).\n",
    "          s = math_ops.reduce_sum(\n",
    "              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "          print('s', s)\n",
    "          a = nn_ops.softmax(s)\n",
    "          print('a', a)\n",
    "          # Now calculate the attention-weighted vector d.\n",
    "          d = math_ops.reduce_sum(\n",
    "              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n",
    "              [1, 2])\n",
    "          print('d', d)\n",
    "          ds.append(array_ops.reshape(d, [-1, attn_size]))\n",
    "      return ds\n",
    "\n",
    "    outputs = []\n",
    "    prev = None\n",
    "    batch_attn_size = array_ops.pack([batch_size, attn_size])\n",
    "    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n",
    "             for _ in xrange(num_heads)]\n",
    "    for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "      a.set_shape([None, attn_size])\n",
    "    if initial_state_attention:\n",
    "      attns = attention(initial_state)\n",
    "    for i, inp in enumerate(decoder_inputs):\n",
    "      if i > 0:\n",
    "        variable_scope.get_variable_scope().reuse_variables()\n",
    "      # If loop_function is set, we use it instead of decoder_inputs.\n",
    "      if loop_function is not None and prev is not None:\n",
    "        with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "          inp = loop_function(prev, i)\n",
    "      # Merge input and previous attentions into one vector of the right size.\n",
    "      input_size = inp.get_shape().with_rank(2)[1]\n",
    "      if input_size.value is None:\n",
    "        raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "      x = linear([inp] + attns, input_size, True)\n",
    "      # Run the RNN.\n",
    "      cell_output, state = cell(x, state)\n",
    "      # Run the attention mechanism.\n",
    "      if i == 0 and initial_state_attention:\n",
    "        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                           reuse=True):\n",
    "          attns = attention(state)\n",
    "      else:\n",
    "        attns = attention(state)\n",
    "\n",
    "      with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "        output = linear([cell_output] + attns, output_size, True)\n",
    "      if loop_function is not None:\n",
    "        prev = output\n",
    "      outputs.append(output)\n",
    "\n",
    "  return outputs, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = melt.create_rnn_cell(4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "emb_dim = 6\n",
    "init_width = 0.5 / emb_dim\n",
    "#emb = melt.variable.init_weights_uniform([vocab_size, emb_dim], -init_width, init_width)\n",
    "emb = tf.constant([[-0.0454044 ,  0.07558767,  0.06434789,  0.04944561,  0.04671062,\n",
    "        -0.06196741],\n",
    "       [ 0.04754589, -0.03475843, -0.03286489,  0.00497814, -0.05656481,\n",
    "        -0.07599609],\n",
    "       [-0.06159163, -0.00535063, -0.03759231, -0.04672422,  0.01091411,\n",
    "         0.02889993],\n",
    "       [-0.05034878, -0.04895053,  0.07128759,  0.04060432,  0.07238931,\n",
    "         0.03129234],\n",
    "       [-0.04462979, -0.00026041,  0.03161035, -0.01818546,  0.06576461,\n",
    "         0.04641552]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = tf.constant([[1,1,3,2, 3]])\n",
    "inputs = tf.nn.embedding_lookup(emb, seq)\n",
    "seq_length = melt.length(seq) - 1\n",
    "encode_feature, state = melt.rnn.encode(\n",
    "          cell, \n",
    "          inputs, \n",
    "          seq_length, \n",
    "          encode_method=0,\n",
    "          output_method=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope <tensorflow.python.ops.variable_scope.VariableScope object at 0x8a66d90> attention_decoder\n",
      "Tensor(\"attention_decoder/abc/read:0\", shape=(1, 3), dtype=float32) attention_decoder/abc:0\n",
      "k Tensor(\"attention_decoder/AttnW_0/read:0\", shape=(1, 1, 4, 4), dtype=float32) attention_decoder/AttnW_0:0\n",
      "state.name RNN/while/Exit_2:0\n",
      "query_list [<tf.Tensor 'attention_decoder/LSTMCell/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder/LSTMCell/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder/concat:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder/Attention_0/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder/Conv2D:0\", shape=(1, 5, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder/Attention_0/add_1:0\", shape=(?, 5, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder/Attention_0/Sum:0\", shape=(?, 5), dtype=float32)\n",
      "a Tensor(\"attention_decoder/Attention_0/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "d Tensor(\"attention_decoder/Attention_0/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder/LSTMCell_1/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder/LSTMCell_1/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder/concat_1:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder/Attention_0_1/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder/Conv2D:0\", shape=(1, 5, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder/Attention_0_1/add_1:0\", shape=(?, 5, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder/Attention_0_1/Sum:0\", shape=(?, 5), dtype=float32)\n",
      "a Tensor(\"attention_decoder/Attention_0_1/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "d Tensor(\"attention_decoder/Attention_0_1/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder/LSTMCell_2/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder/LSTMCell_2/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder/concat_2:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder/Attention_0_2/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder/Conv2D:0\", shape=(1, 5, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder/Attention_0_2/add_1:0\", shape=(?, 5, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder/Attention_0_2/Sum:0\", shape=(?, 5), dtype=float32)\n",
      "a Tensor(\"attention_decoder/Attention_0_2/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "d Tensor(\"attention_decoder/Attention_0_2/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder/LSTMCell_3/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder/LSTMCell_3/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder/concat_3:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder/Attention_0_3/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder/Conv2D:0\", shape=(1, 5, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder/Attention_0_3/add_1:0\", shape=(?, 5, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder/Attention_0_3/Sum:0\", shape=(?, 5), dtype=float32)\n",
      "a Tensor(\"attention_decoder/Attention_0_3/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "d Tensor(\"attention_decoder/Attention_0_3/Sum_1:0\", shape=(?, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_seq = tf.constant([[1,1,3,4]])\n",
    "decoder_inputs = tf.nn.embedding_lookup(emb, decoder_seq)\n",
    "initial_state = state\n",
    "attention_state = encode_feature\n",
    "decoder_inputs = [tf.squeeze(x, 1) for x in tf.split(1, 4, decoder_inputs)]\n",
    "outputs, final_state = attention_decoder(decoder_inputs, initial_state, attention_state, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_featre Tensor(\"RNN_1/transpose:0\", shape=(1, 6, 4), dtype=float32)\n",
      "scope <tensorflow.python.ops.variable_scope.VariableScope object at 0x8ef6150> attention_decoder\n",
      "Tensor(\"attention_decoder/abc/read:0\", shape=(1, 3), dtype=float32) attention_decoder/abc:0\n",
      "k Tensor(\"attention_decoder/AttnW_0/read:0\", shape=(1, 1, 4, 4), dtype=float32) attention_decoder/AttnW_0:0\n",
      "state.name RNN_1/while/Exit_2:0\n",
      "query_list [<tf.Tensor 'attention_decoder_1/LSTMCell/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder_1/LSTMCell/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder_1/concat:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder_1/Attention_0/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder_1/Conv2D:0\", shape=(1, 6, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder_1/Attention_0/add_1:0\", shape=(?, 6, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder_1/Attention_0/Sum:0\", shape=(?, 6), dtype=float32)\n",
      "a Tensor(\"attention_decoder_1/Attention_0/Softmax:0\", shape=(?, 6), dtype=float32)\n",
      "d Tensor(\"attention_decoder_1/Attention_0/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder_1/LSTMCell_1/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder_1/LSTMCell_1/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder_1/concat_1:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder_1/Attention_0_1/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder_1/Conv2D:0\", shape=(1, 6, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder_1/Attention_0_1/add_1:0\", shape=(?, 6, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder_1/Attention_0_1/Sum:0\", shape=(?, 6), dtype=float32)\n",
      "a Tensor(\"attention_decoder_1/Attention_0_1/Softmax:0\", shape=(?, 6), dtype=float32)\n",
      "d Tensor(\"attention_decoder_1/Attention_0_1/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder_1/LSTMCell_2/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder_1/LSTMCell_2/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder_1/concat_2:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder_1/Attention_0_2/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder_1/Conv2D:0\", shape=(1, 6, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder_1/Attention_0_2/add_1:0\", shape=(?, 6, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder_1/Attention_0_2/Sum:0\", shape=(?, 6), dtype=float32)\n",
      "a Tensor(\"attention_decoder_1/Attention_0_2/Softmax:0\", shape=(?, 6), dtype=float32)\n",
      "d Tensor(\"attention_decoder_1/Attention_0_2/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder_1/LSTMCell_3/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder_1/LSTMCell_3/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder_1/concat_3:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder_1/Attention_0_3/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder_1/Conv2D:0\", shape=(1, 6, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder_1/Attention_0_3/add_1:0\", shape=(?, 6, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder_1/Attention_0_3/Sum:0\", shape=(?, 6), dtype=float32)\n",
      "a Tensor(\"attention_decoder_1/Attention_0_3/Softmax:0\", shape=(?, 6), dtype=float32)\n",
      "d Tensor(\"attention_decoder_1/Attention_0_3/Sum_1:0\", shape=(?, 4), dtype=float32)\n",
      "query_list [<tf.Tensor 'attention_decoder_1/LSTMCell_4/add_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'attention_decoder_1/LSTMCell_4/mul_2:0' shape=(?, 4) dtype=float32>]\n",
      "query Tensor(\"attention_decoder_1/concat_4:0\", shape=(?, 8), dtype=float32)\n",
      "y Tensor(\"attention_decoder_1/Attention_0_4/Reshape:0\", shape=(?, 1, 1, 4), dtype=float32)\n",
      "hidden_features[0] Tensor(\"attention_decoder_1/Conv2D:0\", shape=(1, 6, 1, 4), dtype=float32)\n",
      "z Tensor(\"attention_decoder_1/Attention_0_4/add_1:0\", shape=(?, 6, 1, 4), dtype=float32)\n",
      "s Tensor(\"attention_decoder_1/Attention_0_4/Sum:0\", shape=(?, 6), dtype=float32)\n",
      "a Tensor(\"attention_decoder_1/Attention_0_4/Softmax:0\", shape=(?, 6), dtype=float32)\n",
      "d Tensor(\"attention_decoder_1/Attention_0_4/Sum_1:0\", shape=(?, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#melt.reuse_variables()\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "seq = tf.constant([[1,1,3,2, 3, 2]])\n",
    "inputs = tf.nn.embedding_lookup(emb, seq)\n",
    "seq_length = melt.length(seq) - 1\n",
    "encode_feature, state = melt.rnn.encode(\n",
    "          cell, \n",
    "          inputs, \n",
    "          seq_length, \n",
    "          encode_method=0,\n",
    "          output_method=3)\n",
    "print('encode_featre', encode_feature)\n",
    "l = [1,1,3,4, 3]\n",
    "decoder_seq = tf.constant([l])\n",
    "decoder_inputs = tf.nn.embedding_lookup(emb, decoder_seq)\n",
    "initial_state = state\n",
    "attention_state = encode_feature\n",
    "decoder_inputs = [tf.squeeze(x, 1) for x in tf.split(1, len(l), decoder_inputs)]\n",
    "#melt.reuse_variables()\n",
    "\n",
    "outputs, final_state = attention_decoder(decoder_inputs, initial_state, attention_state, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                         tf.local_variables_initializer())\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04754589, -0.03475843, -0.03286489,  0.00497814, -0.05656481,\n",
       "        -0.07599609]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs[0].eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04754589, -0.03475843, -0.03286489,  0.00497814, -0.05656481,\n",
       "         -0.07599609],\n",
       "        [ 0.04754589, -0.03475843, -0.03286489,  0.00497814, -0.05656481,\n",
       "         -0.07599609],\n",
       "        [-0.05034878, -0.04895053,  0.07128759,  0.04060432,  0.07238931,\n",
       "          0.03129234],\n",
       "        [-0.06159163, -0.00535063, -0.03759231, -0.04672422,  0.01091411,\n",
       "          0.02889993],\n",
       "        [-0.05034878, -0.04895053,  0.07128759,  0.04060432,  0.07238931,\n",
       "          0.03129234],\n",
       "        [-0.06159163, -0.00535063, -0.03759231, -0.04672422,  0.01091411,\n",
       "          0.02889993]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0454044 ,  0.07558767,  0.06434789,  0.04944561,  0.04671062,\n",
       "        -0.06196741],\n",
       "       [ 0.04754589, -0.03475843, -0.03286489,  0.00497814, -0.05656481,\n",
       "        -0.07599609],\n",
       "       [-0.06159163, -0.00535063, -0.03759231, -0.04672422,  0.01091411,\n",
       "         0.02889993],\n",
       "       [-0.05034878, -0.04895053,  0.07128759,  0.04060432,  0.07238931,\n",
       "         0.03129234],\n",
       "       [-0.04462979, -0.00026041,  0.03161035, -0.01818546,  0.06576461,\n",
       "         0.04641552]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.17767451e-03,  -7.36213569e-03,  -1.53179804e-03,\n",
       "          -2.59350846e-03],\n",
       "        [  9.83432122e-03,  -1.14350067e-02,  -2.33986042e-03,\n",
       "          -5.12735406e-03],\n",
       "        [  1.28925231e-03,  -9.34195053e-03,  -1.46743150e-05,\n",
       "          -4.34792601e-03],\n",
       "        [ -1.25635443e-02,  -6.64551603e-03,  -8.03328864e-03,\n",
       "          -1.25908032e-02],\n",
       "        [ -1.34159653e-02,  -7.22592324e-03,  -5.32426918e-03,\n",
       "          -1.06535060e-02],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "           0.00000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_feature.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'RNN_1/while/Exit_2:0' shape=(?, 4) dtype=float32>, h=<tf.Tensor 'RNN_1/while/Exit_3:0' shape=(?, 4) dtype=float32>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=4, h=4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0268254 , -0.01475926, -0.01074809, -0.02152603]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01341597, -0.00722592, -0.00532427, -0.01065351]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'attention_decoder_1/AttnOutputProjection/add:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'attention_decoder_1/AttnOutputProjection_1/add:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'attention_decoder_1/AttnOutputProjection_2/add:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'attention_decoder_1/AttnOutputProjection_3/add:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'attention_decoder_1/AttnOutputProjection_4/add:0' shape=(?, 4) dtype=float32>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00701265, -0.00745942, -0.00434384,  0.00290774]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00697165, -0.00903419, -0.00193713,  0.00177378]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
